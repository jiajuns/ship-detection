{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from skimage.morphology import label\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage.morphology import binary_opening, disk, label\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p))\n",
    "    \n",
    "class Unet34(nn.Module):\n",
    "    def __init__(self, rn):\n",
    "        super().__init__()\n",
    "        self.rn = rn\n",
    "        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.rn(x))\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x)\n",
    "        return x[:,0]\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()\n",
    "            \n",
    "class UnetModel():\n",
    "    def __init__(self,model,name='unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n",
    "        return lgs + [children(self.model)[1:]]\n",
    "\n",
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = resnet34\n",
    "cut,lr_cut = model_meta[f]\n",
    "\n",
    "def get_base():\n",
    "    layers = cut_model(f(True), cut)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_base = get_base()\n",
    "m = to_gpu(Unet34(m_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('data', 'train_ship_segmentations.csv'))\n",
    "TRAIN_DR = 'data/train'\n",
    "# TRAIN_LABEL_DN = 'data/train_mask'\n",
    "# x_names = np.array(glob(str(Path(TRAIN_DN)/f'*.jpg')))[:10000]\n",
    "# y_names = np.array(glob(str(Path(TRAIN_LABEL_DN)/f'*.jpg')))[:10000]\n",
    "\n",
    "# val_idxs = list(range(500))\n",
    "# ((val_x, trn_x), (val_y, trn_y)) = split_by_idx(val_idxs, x_names, y_names)\n",
    "\n",
    "val_df = df[:1000]\n",
    "train_df = df[1000:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_rle_encode(img):\n",
    "    labels = label(img[:, :, 0])\n",
    "    return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.float32)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.float32)\n",
    "    #if isinstance(in_mask_list, list):\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(dataframe, bs):\n",
    "    batch_image_arr = []\n",
    "    batch_label_arr = []\n",
    "    cnt = 0\n",
    "    \n",
    "    all_batches = list(dataframe.groupby('ImageId'))\n",
    "    for c_img_id, c_masks in all_batches:\n",
    "        mask_arr = masks_as_image(c_masks['EncodedPixels'].values)\n",
    "        image_arr = open_image(os.path.join(TRAIN_DR, c_img_id))\n",
    "        image_arr = image_arr * 2 - 1\n",
    "        batch_image_arr.append(image_arr.T)\n",
    "        batch_label_arr.append(mask_arr.T)\n",
    "        cnt += 1\n",
    "        if cnt >= bs:\n",
    "            cnt = 0\n",
    "            yield np.array(batch_image_arr), np.array(batch_label_arr)\n",
    "            batch_image_arr, batch_label_arr = [], []\n",
    "    if len(batch_image_arr) and len(batch_label_arr):\n",
    "        yield np.array(batch_image_arr), np.array(batch_label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADUtJREFUeJzt3X+s3XV9x/Hn21LaDQellZEqZGBsIP2HljUKwSyOhgnM4P5ghsaoMU26P9gC0cSV7Y9lyf7Qf0RMFrIGdLgwEatMQwgVC2bZH1ZAKkhLpTAJRaCK5ccw0zHf++N8bjnvu0vv9/44Ped++3wkJ+d8P9/vvefzzcl99fs959vzisxEkqa8bdwTkDRZDAVJhaEgqTAUJBWGgqTCUJBUjCQUIuLyiDgQEQcjYvsonkPSaMRiX6cQEcuAnwCXAYeAB4EtmblvUZ9I0kiM4kjhvcDBzHw6M38D3AF8eATPI2kEThrB73wX8OzQ8iHgfcf6gZNjRa7klBFMRdKU1zjyi8w8Y7btRhEKnUTENmAbwEp+l/fF5nFNRTohfDd3PtNlu1GcPjwHnD20fFYbKzJzR2ZuysxNy1kxgmlImo9RhMKDwLqIODciTgauAb49gueRNAKLfvqQmW9ExF8Cu4BlwJcy8/HFfh5JozGS9xQy8x7gnlH8bkmj5RWNkgpDQVJhKEgqDAVJhaEgqTAUJBWGgqTCUJBUGAqSCkNBUmEoSCoMBUmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFScWsoRARX4qIwxHx46Gx1RFxX0Q82e5Pb+MREV9sxbKPRsSFo5y8pMXX5Ujhn4HLp41tB3Zn5jpgd1sGuAJY127bgJsXZ5qSjpdZQyEz/x345bThDwO3tce3AX82NP6VHPg+sCoi1i7WZCWN3nzfUzgzM59vj18AzmyPZyqXfddMvyAitkXEQxHx0P/w63lOQ9JiW/AbjZmZQM7j5+ySlCbQfEPhxanTgnZ/uI13KpeVNLnmGwrfBj7RHn8C+NbQ+MfbpxAXAa8MnWZIWgJm7ZKMiK8CHwDeERGHgL8DPgvcGRFbgWeAj7TN7wGuBA4CvwI+OYI5SxqhWUMhM7e8xarNM2ybwLULnZSk8fGKRkmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFSYWhIKkwFCQVhoKkwlCQVBgKkgpDQVJhKEgqDAVJhaEgqTAUJBVduiTPjogHImJfRDweEde1cfskpR7qcqTwBvDpzFwPXARcGxHrsU9S6qUuXZLPZ+YP2+PXgP0MquDsk5R6aE7vKUTEOcBGYA8L7JO0S1KaTJ1DISLeDnwDuD4zXx1eN58+SbskpcnUKRQiYjmDQLg9M7/Zhu2TlHqoy6cPAdwK7M/Mzw+tsk9S6qFZa+OAS4CPAY9FxN429jfYJyn1Upcuyf8A4i1W2ycp9YxXNEoqDAVJhaEgqTAUJBWGgqTCUJBUGAqSCkNBUmEoSCoMBUmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFSYWhIKno8m3OKyPiBxHxo9Yl+fdt/NyI2NM6I78WESe38RVt+WBbf85od0HSYupypPBr4NLMvADYAFzevrr9c8CNmfke4AiwtW2/FTjSxm9s20laIrp0SWZm/ldbXN5uCVwK7Gzj07skpzomdwKbW3eEpCWga0PUstb5cBi4D3gKeDkz32ibDPdFHu2SbOtfAdbM8DvtkpQmUKdQyMz/zcwNDCrg3gucv9AntktSmkxz+vQhM18GHgAuZlAxP1UmM9wXebRLsq0/DXhpUWYraeS6fPpwRkSsao9/B7gM2M8gHK5um03vkpzqmLwauL+1RklaArp0Sa4FbouIZQxC5M7MvDsi9gF3RMQ/AI8wKKGl3f9LRBwEfglcM4J5SxqRLl2SjwIbZxh/msH7C9PH/xv480WZnaTjzisaJRWGgqTCUJBUGAqSCkNBUmEoSCoMBUmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFSYWhIKkwFCQVhoKkwlCQVHQOhVYI80hE3N2W7ZKUemguRwrXMfhq9yl2SUo91LU27izgT4Fb2nJgl6TUS12PFL4AfAb4bVtewwK7JCVNpi4NUR8CDmfmw4v5xBbMSpOpS0PUJcBVEXElsBI4FbiJ1iXZjgZm6pI8dKwuyczcAewAODVWWysnTYhZjxQy84bMPCszz2FQAXd/Zn4UuySlXlrIdQp/DXyqdUauoXZJrmnjnwK2L2yKko6nLqcPR2Xm94Dvtcd2SUo95BWNkgpDQVJhKEgqDAVJhaEgqTAUJBWGgqTCUJBUGAqSCkNBUmEoSCoMBUmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFSUXX2rifRsRjEbE3Ih5qY6sj4r6IeLLdn97GIyK+2ApmH42IC0e5A5IW11yOFP44Mzdk5qa2vB3YnZnrgN28+VXuVwDr2m0bcPNiTVbS6C3k9GG4SHZ6wexXcuD7DJqk1i7geSQdR11DIYHvRMTDEbGtjZ2Zmc+3xy8AZ7bHRwtmm+Hy2aPskpQmU9cymPdn5nMR8fvAfRHxxPDKzMyImFM1nF2S0mTqdKSQmc+1+8PAXQyaoV6cOi1o94fb5lMFs1OGy2clTbguVfSnRMTvTT0G/gT4MbVIdnrB7MfbpxAXAa8MnWZImnBdTh/OBO6KiKnt/zUz742IB4E7I2Ir8Azwkbb9PcCVwEHgV8AnF33WkkZm1lBoRbIXzDD+ErB5hvEErl2U2Uk67ryiUVJhKEgqDAVJhaEgqTAUJBWGgqTCUJBUGAqSCkNBUmEoSCoMBUmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFSUXXLslVEbEzIp6IiP0RcbFdklI/dT1SuAm4NzPPZ/AlrvuxS1LqpS69D6cBfwTcCpCZv8nMl7FLUuqlLkcK5wI/B74cEY9ExC2tFMYuSamHuoTCScCFwM2ZuRF4nTdPFYCjXQ9z7pLMzE2ZuWk5K+byo5JGqEsoHAIOZeaetryTQUjYJSn10KyhkJkvAM9GxHltaDOwD7skpV7qWkX/V8DtEXEy8DSDfsi3YZek1DudQiEz9wKbZlhll6TUM17RKKkwFCQVhoKkwlAYgV0/2zvuKUjzZigsMgNBS13XjyTV0QffuWHcU5AWxCMFSYWhIKkwFCQVhoKkwlCQVBgKkgpD4Tjw2gUtJYbCceC1C1pKDAVJhaEgqTAUJBWGgqTCUJBUdGmIOi8i9g7dXo2I6+2SlPqpy1e8H8jMDZm5AfhDBt/QfBd2SUq9NNfTh83AU5n5DHZJSr0011C4Bvhqe7ygLklJk6lzKLQimKuAr09fN58uSQtmpck0l69juwL4YWa+2JZfjIi1mfn8fLokM3MHsAPg1Fg9p0BZqOH/i+AlyFI1l9OHLbx56gA96ZLc9bO9R2+SOh4pRMQpwGXAXwwNf5Yl2CU50x+/RwvSm7p2Sb4OrJk29hJ2SUq9c8J9xbtHBdKxeZmzpMJQkFQYCpIKQ2Ge/AhTfWUozNHwNQ0Gg/rIUJiDmULAC5/UN4ZCB2/1hz/88abBoL4wFGbxVn/sH3znhv+3zmBQH5xwFy91NVsYTF/vRVHqC48U5mCmo4OpcakvPFKY5lh/9AaCTgSGwjHM9kaigaA+MhSmmekP3fcPdCKJwf90HvMkIl4DDox7HiP2DuAX457ECPV9/2Dp7+MfZOYZs200KUcKBzJz07gnMUoR8VCf97Hv+wcnxj6Cnz5ImsZQkFRMSijsGPcEjoO+72Pf9w9OjH2cjDcaJU2OSTlSkDQhxh4KEXF5RBxoLdXbZ/+JyRMRZ0fEAxGxLyIej4jr2njvmrkjYllEPBIRd7flcyNiT9uXr7UmMSJiRVs+2NafM855dxERqyJiZ0Q8ERH7I+LiPr6GsxlrKETEMuAfGbRPrQe2RMT6cc5pnt4APp2Z64GLgGvbfvSxmfs6YP/Q8ueAGzPzPcARYGsb3wocaeM3tu0m3U3AvZl5PnABg/3s42t4bJk5thtwMbBraPkG4IZxzmmR9utbDMpzDgBr29haBtdjAPwTsGVo+6PbTfKNQQXgbuBS4G4gGFzMc9L01xPYBVzcHp/Utotx78Mx9u004D+nz7Fvr2GX27hPH3rXUN0OkzcCe+hfM/cXgM8Av23La4CXM/ONtjy8H0f3sa1/hWmFQhPmXODnwJfb6dEtrRmtb6/hrMYdCr0SEW8HvgFcn5mvDq/LwT8nS/ajnoj4EHA4Mx8e91xG5CTgQuDmzNwIvM6bpwrA0n8Nuxp3KHRqqF4KImI5g0C4PTO/2YZfbI3czKeZe8JcAlwVET8F7mBwCnETsCoipi6XH96Po/vY1p8GvHQ8JzxHh4BDmbmnLe9kEBJ9eg07GXcoPAisa+9gnwxcw6C1ekmJiABuBfZn5ueHVvWimRsgM2/IzLMy8xwGr9P9mflR4AHg6rbZ9H2c2ver2/YT+69sZr4APBsR57WhzcA+evQadjbuNzUYNFT/BHgK+Ntxz2ee+/B+BoeVjwJ72+1KBufQu4Enge8Cq9v2weBTl6eAx4BN496HOe7vB4C72+N3Az9g0DL+dWBFG1/Zlg+29e8e97w77NcG4KH2Ov4bcHpfX8Nj3byiUVIx7tMHSRPGUJBUGAqSCkNBUmEoSCoMBUmFoSCpMBQkFf8Hp3y1XbLRJCUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_image(image_name, dataframe):\n",
    "    all_batches = list(dataframe.groupby('ImageId'))\n",
    "    for c_img_id, c_masks in all_batches:\n",
    "        if c_img_id == image_name:\n",
    "            mask_arr = masks_as_image(c_masks['EncodedPixels'].values)\n",
    "            image_arr = open_image(os.path.join(TRAIN_DR, c_img_id))\n",
    "            break\n",
    "    plt.imshow(mask_arr[:,:])\n",
    "\n",
    "find_image('00021ddc3.jpg', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, trn_df, val_df, num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        pbar = tqdm(total=int(len(trn_df)/6)+int(len(val_df)/3))\n",
    "        trainloader = dataloader(trn_df, bs=6)\n",
    "        valloader = dataloader(val_df, bs=3)\n",
    "        for x, y in trainloader:\n",
    "            x = Variable(torch.from_numpy(x).cuda())\n",
    "            y = Variable(torch.from_numpy(y).cuda())\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(x)\n",
    "            loss = loss_fn(scores, y)\n",
    "            train_loss.append(loss.data[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_description('train loss = {0:.5f}, val loss = nan'.format(loss.data[0]))\n",
    "            pbar.update(1)\n",
    "        train_mean_loss = np.mean(train_loss)\n",
    "        gc.collect()\n",
    "        \n",
    "        val_loss = []\n",
    "        for t, (x, y) in enumerate(valloader):\n",
    "            x = Variable(torch.from_numpy(x).cuda())\n",
    "            y = Variable(torch.from_numpy(y).cuda())\n",
    "            scores = model(x)\n",
    "            loss = loss_fn(scores, y)\n",
    "            val_loss.append(loss.data[0])\n",
    "            pbar.set_description('train loss = {0:.5f}, val loss = {1:.5f}'.format(train_mean_loss, np.mean(val_loss)))\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        del x, y, scores, loss\n",
    "        gc.collect()\n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.cuda())\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.5f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2. * ((pred*targs).sum()+1e-8)/ ((pred+targs).sum()+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(m.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.00724, val loss = 0.00426:  80%|███████▉  | 4129/5166 [28:55<07:15,  2.38it/s]\n",
      "train loss = 0.00417, val loss = 0.00343:  80%|███████▉  | 4129/5166 [28:36<07:11,  2.41it/s]\n",
      "train loss = 0.00356, val loss = 0.00315:  80%|███████▉  | 4129/5166 [28:31<07:09,  2.41it/s]\n",
      "train loss = 0.00322, val loss = 0.00277:  80%|███████▉  | 4129/5166 [28:32<07:10,  2.41it/s]\n",
      "train loss = 0.00296, val loss = 0.00241:  80%|███████▉  | 4129/5166 [28:31<07:09,  2.41it/s]\n",
      "train loss = 0.00276, val loss = 0.00237:  80%|███████▉  | 4129/5166 [28:31<07:09,  2.41it/s]\n",
      "train loss = 0.00256, val loss = 0.00219:  80%|███████▉  | 4129/5166 [28:34<07:10,  2.41it/s]\n",
      "train loss = 0.00239, val loss = 0.00212:  80%|███████▉  | 4129/5166 [28:32<07:10,  2.41it/s]\n",
      "train loss = 0.00219, val loss = 0.00213:  80%|███████▉  | 4129/5166 [28:32<07:10,  2.41it/s]\n",
      "train loss = 0.00206, val loss = 0.00210:  80%|███████▉  | 4129/5166 [28:32<07:10,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "train(m, loss_fn, optimizer, train_df, val_df, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b7cde12ab04e38837c825406791428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=88500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_paths = './data/test/'\n",
    "out_pred_rows = []\n",
    "for c_path in tqdm_notebook(glob(os.path.join(test_paths, '*.jpg'))):\n",
    "    c_img = open_image(c_path)\n",
    "    c_img = c_img * 2 - 1\n",
    "    c_img = c_img.T\n",
    "    c_img = np.expand_dims(c_img, 0)\n",
    "    cur_seg = m(Variable(torch.from_numpy(c_img).cuda()))\n",
    "    cur_seg = cur_seg.cpu().data.numpy()\n",
    "    cur_seg = np.expand_dims(cur_seg[0, :, :].T, axis=0)\n",
    "    cur_seg = binary_opening(cur_seg>0, np.expand_dims(disk(2), -1))\n",
    "    cur_rles = multi_rle_encode(cur_seg)\n",
    "    if len(cur_rles)>0:\n",
    "        for c_rle in cur_rles:\n",
    "            out_pred_rows += [{'ImageId': os.path.basename(c_path), 'EncodedPixels': c_rle}]\n",
    "    else:\n",
    "        out_pred_rows += [{'ImageId': os.path.basename(c_path), 'EncodedPixels': None}]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(out_pred_rows)[['ImageId', 'EncodedPixels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c26e42455.jpg</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af0d4aa65.jpg</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f2a4ea559.jpg</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4be39ce24.jpg</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01d83dd8a.jpg</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId EncodedPixels\n",
       "0  c26e42455.jpg          None\n",
       "1  af0d4aa65.jpg          None\n",
       "2  f2a4ea559.jpg          None\n",
       "3  4be39ce24.jpg          None\n",
       "4  01d83dd8a.jpg          None"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/test/e42e01752.jpg'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 768, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_seg = m(Variable(torch.from_numpy(c_img).cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_seg.cpu().data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_seg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread('data/dba4b574c.jpg', cv2.IMREAD_UNCHANGED).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3442f66668>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADMRJREFUeJzt3X/sXXV9x/Hn2/7ccFKorOmErDU2kP5DyxqlwSyOhg2Ywf3BDI1RY5p0f7AFookr2x/Lkv2h/4iYLGQEdLgwEatM0xAQC2bZH1aKVJCWSmESyoAqlh/DTNf53h/38y33/U3t93x/3N7b0+cjufme8znn+72fk5u8es69p/cVmYkkTXnbuCcgabIYCpIKQ0FSYShIKgwFSYWhIKkYSShExJURcTAiDkXEjlE8h6TRiIW+TyEiFgE/Bq4ADgOPAFszc/+CPpGkkRjFmcJ7gUOZ+Wxm/gq4G/jQCJ5H0ggsHsHffBfw/ND6YeB9J/uFpbEsl3PWCKYiacobHP1ZZp43036jCIVOImI7sB1gOb/N+2LLuKYinRG+kzuf67LfKC4fXgAuGFo/v40VmXlbZm7KzE1LWDaCaUiai1GEwiPAuohYGxFLgeuAb43geSSNwIJfPmTmsYj4S+ABYBHwxcx8cqGfR9JojOQ9hcy8D7hvFH9b0mh5R6OkwlCQVBgKkgpDQVJhKEgqDAVJhaEgqTAUJBWGgqTCUJBUGAqSCkNBUmEoSCoMBUmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSMWMoRMQXI+JIRPxoaOzciHgwIp5uP89p4xERX2jFso9HxCWjnLykhdflTOGfgSunje0AdmfmOmB3Wwe4CljXHtuBWxdmmpJOlRlDITP/Hfj5tOEPAXe25TuBPxsa/3IOfA9YERGrF2qykkZvru8prMrMF9vyS8Cqtnyictl3negPRMT2iNgbEXv/l1/OcRqSFtq832jMzARyDr9nl6Q0geYaCi9PXRa0n0faeKdyWUmTa66h8C3g423548A3h8Y/1j6FuBR4begyQ9JpYMYuyYj4CvAB4J0RcRj4O+AzwD0RsQ14Dvhw2/0+4GrgEPAL4BMjmLOkEZoxFDJz62/YtOUE+yZw/XwnJWl8vKNRUmEoSCoMBUmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFSYWhIKkwFCQVhoKkwlCQVBgKkgpDQVJhKEgqDAVJRZcuyQsi4uGI2B8RT0bEDW3cPkmph7qcKRwDPpWZ64FLgesjYj32SUq91KVL8sXM/EFbfgM4wKAKzj5JqYdm9Z5CRKwBNgJ7mGefpF2S0mTqHAoR8Xbg68CNmfn68La59EnaJSlNpk6hEBFLGATCXZn5jTZsn6TUQ10+fQjgDuBAZn5uaJN9klIPzVgbB1wGfBR4IiL2tbG/wT5JqZe6dEn+BxC/YbN9klLPeEejpMJQkFQYCpIKQ0FSYShIKgwFSYWhIKkwFCQVhoKkwlCQVBgKkgpDQVJhKEgqDAVJhaEgqTAUJBWGgqTCUJBUGAqSii7f5rw8Ir4fET9sXZJ/38bXRsSe1hn51YhY2saXtfVDbfua0R6CpIXU5Uzhl8DlmXkxsAG4sn11+2eBmzPzPcBRYFvbfxtwtI3f3PaTdJro0iWZmfnfbXVJeyRwObCzjU/vkpzqmNwJbGndEZJOA10boha1zocjwIPAM8CrmXms7TLcF3m8S7Jtfw1YeYK/aZekNIE6hUJm/l9mbmBQAfde4KL5PrFdktJkmtWnD5n5KvAwsJlBxfxUmcxwX+TxLsm2/WzglQWZraSR6/Lpw3kRsaIt/xZwBXCAQThc23ab3iU51TF5LfBQa42SdBro0iW5GrgzIhYxCJF7MnNXROwH7o6IfwAeY1BCS/v5LxFxCPg5cN0I5i1pRLp0ST4ObDzB+LMM3l+YPv4/wJ8vyOwknXLe0SipMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFSYWhIKkwFCQVhoKkwlCQVBgKUkcP/Ne+cU/hlOjyX6elM9qZEgZTDAVpBn/yexvGPYVTyssHSYWhIKkwFCQVhoKkonMotEKYxyJiV1u3S1LqodmcKdzA4Kvdp9glKfVQ19q484E/BW5v64FdklIvdT1T+DzwaeDXbX0l8+ySlDSZujREfRA4kpmPLuQTWzArTaYudzReBlwTEVcDy4F3ALfQuiTb2cCJuiQPn6xLMjNvA24DeEeca62cNCFmPFPIzJsy8/zMXMOgAu6hzPwIdklKvTSf+xT+Gvhk64xcSe2SXNnGPwnsmN8UJZ1Ks/oPUZn5XeC7bdkuSamHvKNRUmEoSCoMBUmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFSYWhIKkwFCQVhoKkwlCQVBgKkgpDQVJhKEgqDAVJhaEgqehaG/eTiHgiIvZFxN42dm5EPBgRT7ef57TxiIgvtILZxyPiklEegKSFNZszhT/KzA2Zuamt7wB2Z+Y6YDdvfZX7VcC69tgO3LpQk5U0evO5fBgukp1eMPvlHPgegyap1fN4HkmnUNdQSODbEfFoRGxvY6sy88W2/BKwqi0fL5hthstnj7NLUppMXctg3p+ZL0TE7wIPRsRTwxszMyNiVtVwdklKk6nTmUJmvtB+HgHuZdAM9fLUZUH7eaTtPlUwO2W4fFbShOtSRX9WRPzO1DLwx8CPqEWy0wtmP9Y+hbgUeG3oMkPShOty+bAKuDcipvb/18y8PyIeAe6JiG3Ac8CH2/73AVcDh4BfAJ9Y8FlLGpkZQ6EVyV58gvFXgC0nGE/g+gWZnaRTzjsaJRWGgqTCUJBUGAqSCkNBUmEoSCoMBUmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFSYWhIKkwFCQVhoKkwlCQVHTtklwRETsj4qmIOBARm+2SlPqp65nCLcD9mXkRgy9xPYBdklIvdel9OBv4Q+AOgMz8VWa+il2SUi91OVNYC/wU+FJEPBYRt7dSGLskpR7qEgqLgUuAWzNzI/Amb10qAMe7HmbdJZmZmzJz0xKWzeZXJY1Ql1A4DBzOzD1tfSeDkLBLUuqhGUMhM18Cno+IC9vQFmA/dklKvdS1iv6vgLsiYinwLIN+yLdhl6TUO51CITP3AZtOsMkuSalnvKNRUmEoSCoMBUmFoSCpMBQkFYaCpMJQkFQYCpIKQ0FSYShIKgwFSYWhIKkwFCQVhoKkwlCQVBgKkgpDQVJhKEgqDAVJRZeGqAsjYt/Q4/WIuNEuSamfunzF+8HM3JCZG4A/YPANzfdil6TUS7O9fNgCPJOZz2GXpNRLsw2F64CvtOV5dUlKmkydQ6EVwVwDfG36trl0SVowK02m2ZwpXAX8IDNfbuvz6pK0YFaaTLMJha28dekAdklKvdSpNi4izgKuAP5iaPgz2CUp9U7XLsk3gZXTxl7BLkmpd7yjUVJhKEgqDAVJhaEgqTAUJBWGgqTCUJBUGAqSCkNBUmEoSCoMBUmFoSCpMBQkFYaCpCIG/9N5zJOIeAM4OO55jNg7gZ+NexIj1Pfjg9P/GH8/M8+baadO36dwChzMzE3jnsQoRcTePh9j348PzoxjBC8fJE1jKEgqJiUUbhv3BE6Bvh9j348PzoxjnIw3GiVNjkk5U5A0IcYeChFxZUQcbC3VO2b+jckTERdExMMRsT8inoyIG9p475q5I2JRRDwWEbva+tqI2NOO5autSYyIWNbWD7Xta8Y57y4iYkVE7IyIpyLiQERs7uNrOJOxhkJELAL+kUH71Hpga0SsH+ec5ugY8KnMXA9cClzfjqOPzdw3AAeG1j8L3JyZ7wGOAtva+DbgaBu/ue036W4B7s/Mi4CLGRxnH1/Dk8vMsT2AzcADQ+s3ATeNc04LdFzfZFCecxBY3cZWM7gfA+CfgK1D+x/fb5IfDCoAdwOXA7uAYHAzz+LpryfwALC5LS9u+8W4j+Ekx3Y28J/T59i317DLY9yXD71rqG6nyRuBPfSvmfvzwKeBX7f1lcCrmXmsrQ8fx/FjbNtfY1qh0IRZC/wU+FK7PLq9NaP17TWc0bhDoVci4u3A14EbM/P14W05+OfktP2oJyI+CBzJzEfHPZcRWQxcAtyamRuBN3nrUgE4/V/DrsYdCp0aqk8HEbGEQSDclZnfaMPzauaeMJcB10TET4C7GVxC3AKsiIip2+WHj+P4MbbtZwOvnMoJz9Jh4HBm7mnrOxmERJ9ew07GHQqPAOvaO9hLgesYtFafViIigDuAA5n5uaFNvWnmzsybMvP8zFzD4HV6KDM/AjwMXNt2m36MU8d+bdt/Yv+VzcyXgOcj4sI2tAXYT49ew87G/aYGg4bqHwPPAH877vnM8Rjez+C08nFgX3tczeAaejfwNPAd4Ny2fzD41OUZ4Alg07iPYZbH+wFgV1t+N/B9Bi3jXwOWtfHlbf1Q2/7ucc+7w3FtAPa21/HfgHP6+hqe7OEdjZKKcV8+SJowhoKkwlCQVBgKkgpDQVJhKEgqDAVJhaEgqfh/Sq5rXh0bjvoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.imread('data/dba4b574c.jpg', cv2.IMREAD_UNCHANGED)*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread('data/dba4b574c.jpg', cv2.IMREAD_UNCHANGED).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([367, 367]), array([598, 599]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(cv2.imread('data/dba4b574c.jpg', cv2.IMREAD_UNCHANGED) == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([363, 363, 364, 364, 365, 365, 365, 365, 366, 366, 366, 366, 366, 366, 366, 366, 366, 367, 367, 367,\n",
       "        367, 367, 367, 367, 367, 367, 368, 368, 368, 368, 368, 368, 368, 368, 368, 368, 368, 368, 369, 369,\n",
       "        369, 369, 369, 369, 369, 369, 370, 370, 370, 370]),\n",
       " array([600, 601, 600, 601, 598, 599, 600, 601, 594, 595, 596, 597, 598, 599, 600, 601, 602, 592, 593, 594,\n",
       "        595, 596, 597, 600, 601, 602, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 593, 594,\n",
       "        595, 596, 597, 598, 600, 601, 594, 595, 596, 597]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(cv2.imread('data/dba4b574c.jpg', cv2.IMREAD_UNCHANGED) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
